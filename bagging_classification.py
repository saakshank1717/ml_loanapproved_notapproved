# -*- coding: utf-8 -*-
"""Bagging-Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TXX7_aRv5WT87ZjqUU3fghztsiKk4jmQ
"""

import pandas as pd
df=pd.read_csv("/content/Training Dataset.csv")
df.head()

df.tail()

df.shape

df.info()

df.describe()

df.isnull().sum()

df.duplicated().sum()

df[df.duplicated(subset=['Loan_ID'])]

"""TREATING MISSING VALUES"""

# Fill missing Married values with the most common value ("Yes")
married_mode = df['Married'].mode()[0]
df['Married'] = df['Married'].fillna(married_mode)

# 1. Find the most common number of dependents for each group
mode_married = df[df['Married'] == 'Yes']['Dependents'].mode()[0]
mode_single  = df[df['Married'] == 'No']['Dependents'].mode()[0]

# 2. Fill missing values for Married applicants
df.loc[(df['Dependents'].isnull()) & (df['Married'] == 'Yes'), 'Dependents'] = mode_married

# 3. Fill missing values for Single applicants
df.loc[(df['Dependents'].isnull()) & (df['Married'] == 'No'), 'Dependents'] = mode_single

df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])
df['Self_Employed'] = df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0])
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])

df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].median())

df.isnull().sum()

"""TRANSFORMATION"""

# Convert Binary Categorical to 0 and 1
df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})
df['Married'] = df['Married'].map({'Yes': 1, 'No': 0})
df['Education'] = df['Education'].map({'Graduate': 1, 'Not Graduate': 0})
df['Self_Employed'] = df['Self_Employed'].map({'Yes': 1, 'No': 0})
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})

df.head()

print(df['Dependents'].unique())
df['Dependents'] = df['Dependents'].replace('3+', 3).astype(int)
print(df['Dependents'].unique())

# One-Hot Encoding for Property_Area
df = pd.get_dummies(df, columns=['Property_Area'])

df.head(5)

#df['Loan_Status'].value_counts()
# Create TotalIncome and drop the original two columns
df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']
df.drop(['ApplicantIncome', 'CoapplicantIncome'], axis=1, inplace=True)

"""CORRELATION & HEATMAP"""

import seaborn as sns
import matplotlib.pyplot as plt
# Drop the ID column as it's not needed for the model or correlation
df = df.drop('Loan_ID', axis=1, errors='ignore')

# Now run correlation
corr = df.corr()

# Generate a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# List of columns you want to remove
cols_to_drop = ['Loan_ID', 'Gender', 'Self_Employed']

# Drop the columns
df.drop(columns=cols_to_drop, inplace=True, errors='ignore')

# Verify that they are gone
print("Remaining columns:")
print(df.columns.tolist())

import seaborn as sns
import matplotlib.pyplot as plt

# Set up the figure with two subplots (1 row, 2 columns)
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# 1. Left Plot: TotalIncome (Skyblue)
sns.histplot(df['TotalIncome'], kde=True, ax=axes[0], color='skyblue')
axes[0].set_title('TotalIncome Distribution')

# 2. Right Plot: LoanAmount (Yellow)
sns.histplot(df['LoanAmount'], kde=True, ax=axes[1], color='gold') # 'gold' or 'yellow'
axes[1].set_title('LoanAmount Distribution')

plt.tight_layout()
plt.show()

#DATA SKEWED THEREFORE WE USE LOG TRANSFORMATION
import numpy as np
df['TotalIncome_log'] = np.log(df['TotalIncome'])
df['LoanAmount_log'] = np.log(df['LoanAmount'])

# Set up the figure with two subplots (1 row, 2 columns)
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# 1. Left Plot: TotalIncome (Skyblue)
sns.histplot(df['TotalIncome_log'], kde=True, ax=axes[0], color='skyblue')
axes[0].set_title('TotalIncome_log Distribution')

# 2. Right Plot: LoanAmount (Yellow)
sns.histplot(df['LoanAmount_log'], kde=True, ax=axes[1], color='gold') # 'gold' or 'yellow'
axes[1].set_title('LoanAmount_log Distribution')

plt.tight_layout()
plt.show()

df.head(5)

"""MODEL TRAING AND TESTING"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix

y = df['Loan_Status']
X = df.drop('Loan_Status', axis=1)
print("Features being used:", X.columns.tolist())

#Split TRAIN-TEST
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training shapes: {X_train.shape}, {y_train.shape}")
print(f"Testing shapes: {X_test.shape}, {y_test.shape}")

dtree = DecisionTreeClassifier(
    max_depth=3,
    criterion='entropy',
    #class_weight='balanced', # <--- Add this here
    random_state=42
)

dtree.fit(X_train, y_train)

# Make predictions
predictions = dtree.predict(X_test)

# Calculate Accuracy
accuracy = accuracy_score(y_test, predictions)
print(f"The model accuracy is: {accuracy:.2%}")

cm = confusion_matrix(y_test, predictions)
print("Confusion Matrix:")
print(cm)

print("\nDetailed Report:")
print(classification_report(y_test, predictions))

"""BAGGING"""

from sklearn.ensemble import BaggingClassifier
base_model = DecisionTreeClassifier(max_depth=3, criterion='entropy', class_weight='balanced', random_state=42)
bagging_model = BaggingClassifier(estimator=base_model,
                                  n_estimators=500, # 100 trees voting
                                  random_state=42)





# Train the committee
bagging_model.fit(X_train, y_train)

# Predict on the TEST data
bagging_predictions = bagging_model.predict(X_test)

print(f"Bagging Accuracy: {accuracy_score(y_test, bagging_predictions):.2%}")
print("New Confusion Matrix:")
print(confusion_matrix(y_test, bagging_predictions))